{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of Train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG-TzddmcuYG"
      },
      "source": [
        "# Training a Convolutional Neural Network for a computer vision binary classification task\n",
        "\n",
        "From https://github.com/dsikar/city-dss-cnn-workshop-1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOVEOBUrW2Jb",
        "outputId": "57479296-0864-4e3f-f852-50e1867e38a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get the data\n",
        "# Install PyDrive\n",
        "!pip install PyDrive\n",
        "\n",
        "#Import modules\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Get the shareable link e.g. https://drive.google.com/file/d/19sU5tk8RQY605GGjPJMW8mx3S9HrKaFW/view?usp=sharing\n",
        "# Get the id from the link 1JYdxKasFKNfVqx2Iklj7iv_bwPanmi4j\n",
        "downloaded = drive.CreateFile({'id':\"19sU5tk8RQY605GGjPJMW8mx3S9HrKaFW\"})   \n",
        "# Remember at least what was the file extension\n",
        "downloaded.GetContentFile('datasets.tar.gz')       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMHGfMpKLLr",
        "outputId": "292a2e5d-0253-4274-ee61-ff0ae675d098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# list drive contents\n",
        "!ls -lh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 531M\n",
            "-rw-r--r-- 1 root root 2.7K Nov  5 17:13 adc.json\n",
            "-rw-r--r-- 1 root root 266M Nov  5 17:14 datasets.tar.gz\n",
            "-rw-r--r-- 1 root root 266M Nov  5 17:13 fire_data.tar.gz\n",
            "drwxr-xr-x 1 root root 4.0K Oct 28 16:30 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAzJNJ7nZ5j7"
      },
      "source": [
        "# unpack compressed files\n",
        "!tar xvf datasets.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2XT37wYcqUY",
        "outputId": "7f77e0c5-161a-4da2-b82f-fd18b69de2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# list again to verity\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B9wUXR7JTIh"
      },
      "source": [
        "DATADIR = 'datasets/'\n",
        "CATEGORIES = ['fire', 'nofire']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjzBBu7QJTIk"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbRgxxmXJTIn"
      },
      "source": [
        "IMG_SIZE = 64\n",
        "def create_training_data():\n",
        "    training_data = []\n",
        "    for category in CATEGORIES:  \n",
        "\n",
        "        path = os.path.join(DATADIR,category) \n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=C 1=O\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "              \n",
        "    return training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I13u9gXEJTIq"
      },
      "source": [
        "training_data = create_training_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaM3rhv6JTIt"
      },
      "source": [
        "import random\n",
        "\n",
        "print(len(training_data))\n",
        "random.shuffle(training_data)\n",
        "for sample in training_data[:10]:\n",
        "    print(sample[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jHCOODEJTIw"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    X.append(features)\n",
        "    Y.append(label)\n",
        "\n",
        "# try to get around split error\n",
        "Y = np.asarray(Y)\n",
        "\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "X = X/255.0\n",
        "X.shape[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvQf0pJdJTIz"
      },
      "source": [
        "type(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYOrgGuIJTI3"
      },
      "source": [
        "# # set up image augmentation\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=15,\n",
        "#     horizontal_flip=True,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1\n",
        "#     #zoom_range=0.3\n",
        "#     )\n",
        "# datagen.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk3Yk3ZiJTI5"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=X.shape[1:]))\n",
        "model.add(AveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(AveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(AveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "\n",
        "model.add(Dense(units=2, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI-InvF_JTI9"
      },
      "source": [
        "history = model.fit(X, Y, batch_size=32, epochs=100,validation_split=0.3)\n",
        "# model.fit_generator(datagen.flow(X, Y, batch_size=32),\n",
        "#                     epochs=100,\n",
        "#                     verbose=1)\n",
        "\n",
        "#history = model.fit(X, Y, batch_size=27, epochs=10,validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_HTgRTwJTJB"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNS9HYtbauXq"
      },
      "source": [
        "model.save('firemodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg7DJagKJTJE"
      },
      "source": [
        "!ls -lh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd_tZbhcJTJH"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grxxxEjsJTJK"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1NUB5SlJTJQ"
      },
      "source": [
        "# Using the model in practice\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "# model = tf.keras.models.load_model('TrainedModels/Fire-64x64-color-v7-soft.h5')\n",
        "model = tf.keras.models.load_model('firemodel.h5')\n",
        "\n",
        "# TODO\n",
        "# single prediction fire/no fire\n",
        "\n",
        "# TODO, download model and run locally\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}